{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommenders 4 : Pytorch and Recommenders (~1h)\n",
    "\n",
    "In this practical session, we dive a little more into [pytorch](https://pytorch.org/docs/stable/index.html) and propose to re-implement two classical matrix-factorization models with this neural network toolkit.\n",
    "\n",
    "## Objectives:\n",
    "\n",
    "- (a) See a bit of simple pytorch (~5min)\n",
    "- (b) Discover the \"autograd\" part of pytorch to build a simple baseline (~20min)\n",
    "- (c) Discover the \"nn\" part of pytorch to build a simple matrix factorization algorithm (~20min)\n",
    "- (d) Learn to use a high level framework for pytorch (kind of \"KERAS\" like) to build more complicated algorithms (~15min)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install torch torchvision pytorch-lightning --upgrade\n",
    "# ! pip install matplotlib --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (a) WHAT IS PYTORCH?\n",
    "\n",
    "It’s a Python-based scientific computing package targeted at two sets of audiences:\n",
    "\n",
    "- A replacement for NumPy to use the power of GPUs\n",
    "- a deep learning research platform that provides maximum flexibility and speed\n",
    "\n",
    "### Tensors : the main unit\n",
    "\n",
    "Tensors are similar to NumPy’s ndarrays, with the addition being that Tensors can also be used on a GPU to accelerate computing.\n",
    "\n",
    "\n",
    "## Some useful functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### initialize an empty 4x2 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000e+00, 9.6429e-39],\n",
      "        [2.0826e+23, 5.3639e-08],\n",
      "        [2.0338e+20, 2.6226e-09],\n",
      "        [1.3016e+22, 1.7585e-04]])\n"
     ]
    }
   ],
   "source": [
    "x_empty = torch.empty(4, 2)\n",
    "print(x_empty)  #Tensor is not initialized => contains gibberish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create a 3x2 tensor filled with zeros of type long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0]])\n"
     ]
    }
   ],
   "source": [
    "x0 = torch.zeros(3, 2, dtype=torch.long)\n",
    "print(x0) #Tensor has only zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create a tensor of size 2 with (0 => 5.5) and (1 => 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000])\n"
     ]
    }
   ],
   "source": [
    "x_data = torch.tensor([5.5, 3])\n",
    "print(x_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x_data = torch.tensor(np.array([5.5, 3])) #also works with numpy arrays\n",
    "print(x_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create random 5x3 and 3x5 tensors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9378, 0.1067, 0.3300],\n",
      "        [0.7888, 0.3527, 0.9545],\n",
      "        [0.2996, 0.8375, 0.3158],\n",
      "        [0.6897, 0.2835, 0.5356],\n",
      "        [0.6216, 0.5687, 0.6924]])\n",
      "tensor([[0.3629, 0.9372, 0.5462, 0.5289, 0.4704],\n",
      "        [0.7396, 0.3079, 0.6561, 0.1430, 0.2354],\n",
      "        [0.3311, 0.1487, 0.6326, 0.1668, 0.4655]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5,3)\n",
    "y = torch.rand(3,5)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing works just like numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1067, 0.3527, 0.8375, 0.2835, 0.5687])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,1] #The 2nd column (indexing starts at 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7888, 0.3527, 0.9545],\n",
       "        [0.6897, 0.2835, 0.5356]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[[1,3],:] # the 2nd and 4th row \n",
    "# or x[[1,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "scalar = torch.tensor([1])\n",
    "print(scalar.item()) # Gets the value when tensor is a scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### know the size of a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size() ## equivalent to x.shape in numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### simple addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.9378, 1.1067, 1.3300],\n",
       "        [1.7888, 1.3527, 1.9545],\n",
       "        [1.2996, 1.8375, 1.3158],\n",
       "        [1.6897, 1.2835, 1.5356],\n",
       "        [1.6216, 1.5687, 1.6924]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x+1        # same as x.add(1)\n",
    "x.add_(1)  # inplace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.9620, 2.3546, 2.6258, 1.4049, 1.7912],\n",
       "        [2.2967, 2.3836, 3.1009, 1.4654, 2.0698],\n",
       "        [2.2663, 1.9794, 2.7478, 1.1695, 1.6564],\n",
       "        [2.0709, 2.2071, 2.7364, 1.3333, 1.8119],\n",
       "        [2.3090, 2.2544, 2.9855, 1.3642, 1.9200]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(x,y) # same as x @ y or np.dot(x.numpy(),y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to understand:\n",
    "\n",
    "Pytorch can be a drop-in replacement for numpy. It behaves mostly the same and the API is close.\n",
    "\n",
    "\n",
    "### There are many more creation/operation ops:\n",
    "\n",
    "=> You can have a look at the [torch.Tensor documentation](https://pytorch.org/docs/stable/tensors.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's interesting beyond the \"numpy replacement\": autodiff !\n",
    "\n",
    "Pytorch has Automatic differentiation: You only have to compute a loss function to obtain gradients automatically. How it works is detailed [here](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html#pytorch-tensors-and-autograd)\n",
    "\n",
    "### Let's do 1d-linear regression with the vanilla autodiff !\n",
    "\n",
    "#### (First) we need fake data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x26e260edc40>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAtJklEQVR4nO3deXyU1b3H8c+Z7PuekA0CISQk7ARlEWVxl4ptba9WW/Xaq61Lvba1y21tbW9ve6u3Vm9b2+tStWptrVZxwQURBEWWsCQQErIA2ZPJvm8zc+4fMxkSEiAkk8yS3/v14kXmec7MnMcHvxzOcxaltUYIIYT7MTi7AkIIIcZGAlwIIdyUBLgQQrgpCXAhhHBTEuBCCOGmvCfzy6Kjo3VKSspkfqUQQri9/fv3N2itY04/PqkBnpKSQk5OzmR+pRBCuD2lVNlIx6ULRQgh3JQEuBBCuCkJcCGEcFMS4EII4aYkwIUQwk1JgAshhJuSABdCCDclAS6EEBOooaOXn791lF6T2eGfLQEuhBATpLWrn68+s5e/7i2juK7D4Z8vAS6EEBOgo9fErc/tpdTYwf99NZt5iWEO/45JnUovhBBTQU+/ma8/v4+8ylaeuGkJl8wZtoyJQ0gLXAghHOwvn51k9/EmHv3yQq7ImjZh3yMBLoQQDvZ+fh3zEkPZuChxQr9HAlwIIRyovr2XA+XNXDZ34lreAyTAhRDCgbYVGtEaLs2MnfDvkgAXQohRMpktvLy3nKd3HkdrPWKZD47WkRgeQGZ86ITXR0ahCCHEKGw/ZuS/3img2Ggdz32ioZP/3DgPg0HZy3T3mfmkpJ4blk1HKXWmj3IYCXAhhDiLjl4TD/wjl3eP1JISFcifbl5KbmULf9xeSne/mYe/uABvL2tnxiclDfT0W7h0btyk1E0CXAghzqC8sYuv/2UfpfWdPHBFOv+2eha+3gauyIoj0MeL32wpoqvXzKP/spBAX2+2HK0lxN+bC2dFTkr9zhngSqk/AxsAo9Z6nu1YJPB3IAU4CXxZa908cdUUQojJted4I3e+uB+t4fnbLuCitGj7OaUU965PI9DPm1+8c5QvPNHJEzctYWuBkTXpsfh4Tc7jxdF8y3PAlacd+wGwVWudBmy1vRZCCI+gtebbr+QSEejLprtXDQnvwW6/aCbP3XYBtW09XPX4Tho7+7gsc3K6T2AUAa613gE0nXZ4I/C87efngescWy0hhHCegxUtVLV0c8/a2aREB5217CVzYnjrnotIjQkmyNeLNekTM21+JGPtA4/TWtfYfq4FJu+vHCGEmGDv5NXg62XgsqzRRVtyZCBv3L2Klu4+Qv19Jrh2p4y7o0ZbB0OOPCASUErdoZTKUUrl1NfXj/frhBBiQlksmnfyarh4Tsx5hbGvt4HYEP8JrNlwYw3wOqVUPIDtd+OZCmqtn9RaZ2uts2NiJu+fFkIIMRYHypupbethw4J4Z1flnMYa4G8Ct9h+vgXY5JjqCCGEc72dV4Ovt4FLJ/Fh5FidM8CVUi8DnwHpSqlKpdTtwH8DlymlioFLba+FEMLl9ZksfOGJT3n0g2PDpsObLZrNh2tYmx5DsJ/rT5M5Zw211jee4dR6B9dFCCEmXM7JJg6Ut3CgvIW2HhM/2ZBpnw6fc7IJY3sv1yxIcHItR8f1/4oRQggH2lpoxNfbwI3Lknlu10m6+8x854o5NHf289e95fj7GFifMfErCTqCBLgQYkr5qNDIillRPHRtFmGBvvzv1mL+nlNhP79hQTxBbtB9AhLgQogp5Hh9BycaOrltVQpKKb592RzmJYRS19ZDRJAvkYG+LEgOd3Y1R00CXAgxZXxUaB3xvDb9VBfJ5RO4Z+VEkw0dhBBTxkeFRubEBZMcGejsqjiEBLgQwq2daOjk5qf30NzZd9ZybT397D3RxLoM1x/fPVoS4EIIt/b6gUo+KWmwd4+cyc6iBkwWzfq57jHCZDQkwIUQbm1HcQMAu0obz1pua2EdYQE+LHajh5TnIgEuhHBbLV195FW2oBTsPt54xo2GTWYL24/VsyY9xr79mSfwnCsRQkw5n5Y0YtGwcWECVS3dlDd1jVjuTx+X0tTZx7UL3WOG5WhJgAsh3NbO4npC/L355prZwMjdKEeqWnnsw2I2LIhn/SRtNjxZJMCFEG5Ja82OonpWpUYzJy6Y2BC/YQHe02/m/r8fIirYl19cN89JNZ04EuBCCLdUWt9JdWsPq+dEo5RiZWoUn5UO7Qd/5P1jFBs7ePj6hYQH+jqxthNDAlwI4ZZ2Flt3+Lo4zbpRzIrUKBo6eikxdgDwSXEDz3xygq+tmMElczxzMxmZSi+EcEs7iuqZGR1kn1W5MtW6c/yu0kYig3y5/5VDzI4N5odXzXVmNSeUBLgQwu30mszsPt7El7KT7MeSIwNJigjg05IGPi6qp7Wrn+dvu4AAXy8n1nRiSYALIdzOvhPNdPeb7d0nA1amRvGP/ZVoDT/9XCaZCaFOquHkkD5wIYRbMVs0j3xwjKggX1akRg05tzI1Gq1hXUYst65McU4FJ5G0wIUQbuXF3WXkVrTw2L8sGrbxwuVZcdy9NpXbL5qFUspJNZw8EuBCCKcwmS0A5zW1vaa1m0feP8bqtGg2Lho+qzLQ15sHrshwWB1dnXShCCGc4s4X9vONFw+c13t+uikfk8XCf103f0q0sM9FWuBCiEnX2tXP9qJ6LFpT19ZDXKj/Od/zwu4yPjhaxw+uymB6lGdsyDBe0gIXQky67UVGzBaN1vBWbvVZy/aZLPzo9cM8+MYRVqdFc/tFMyeplq5PAlwIMek+LDASHezLvMRQ3jhUdcZy1S3d3PjUbl7aU86dl8zi2VuX4eNBy8GOl/yXEEKMm8Wi+d+txZxo6Dxn2X6zhe3HjKxNj+W6RYkcqWqzT38f0NFr4jcfHGPdb7ZztLqN339lMT+8aq5HreXtCPJfQwgxbu/l1/LoliKe/fTEsHMPvnGEV/dX2l/vO9lEe4+J9XPjuHZhAgYFbw5qhX94tI41j2zndx+VcFnmND64/2I2LPCsdbwdRQJcCDEuA61vgI+L6oecq2rp5oXdZTz4xhEqbJstbC0w4utlYHVaNLGh/qxMjeaNQ9VordlaUMc3XtxPXKgfr9+1kt/duNhjdpCfCBLgQohx2VJQR2FtO9kzIihr7KKs8VQ3ykcFdYB19uRPNh2xh/SK1Cj7JJxrFyVQ3tTFE9tL+eZLB8hMCOXlO5azeHqEU67HnUiACyGG6Tdb2FXaMOx4dUs3qx/+yN5VorW19Z0SFch/f3EBYF0lcMCHBUZmRAXyvSvT2Xasnt9/VMLJxi4uHbQz/JXzpuHrbeCR948xKzqI52+7gFB/nwm+Qs8gAS6EGOa1/ZV85ak9FNa2DTm++3gjFU3d/Oyto/x00xE+OFpHfnUbd6+dTWpMEMmRAfZulM5eE5+VNnLp3DhuXZlCZnwov9lSBMC6QVubhfr7cP3SJDKmhfDC7RcSEeR5Gy9MlHEFuFLqfqVUvlLqiFLqZaXUuUfjCyFc3p4TTQDkVrQMOZ5f3Yaft4GvXzST5z8r456/HiA5MoDrFieilOKSOTF8VtpIn8nCzuIG+swW1s+NxdvLwC+/MB+lYG58KInhAUM+9xcb5/HufauJCfGbrEv0CGMOcKVUIvAtIFtrPQ/wAm5wVMWEEM6TU2YN8MNVrUOO51e3khEfyo83ZPLLz89Hofj2ZXPsY7MvTouhs8/M/rJmthbUEeLvzbKUSAAWJYfzyPUL+fE1wzdYMBiUTI0fg/FOpfcGApRS/UAgcPYpVUIIl1fX1kNFUzcAh6tOdaForTla3caGhdYhfV+5cDpfXJqIn/epDRNWpEbhbVBsP2Zk2zEja9Jjh0y8uX7pqQ0YxPiNuQWuta4C/gcoB2qAVq31B6eXU0rdoZTKUUrl1NfXn35aCOFER6vb+N3W4iEbAeecbAbggpRICmra6LetGljZ3E1bj4msQZskDA5vgBB/H5bOiOCve8pp6Ogb8rBSON54ulAigI3ATCABCFJK3Xx6Oa31k1rrbK11dkyMZ24sKoS7emlPGb/ZUkR+9amWdk5ZE/4+Bv5lWTJ9JgvFddZZkvnV1u6UrISws37mxXNiaO814WVQrJkjAT6RxvMQ81LghNa6XmvdD/wTWOmYagkhJkNhbTsAbxw8NRNyf1kzC5PCWTw9HIAjtn7wI1VteBkUGdNCzvqZAzvAZ8+IICxQhgNOpPEEeDmwXCkVqKxPH9YDBY6plhBiolksmsIaa8t7U241Zoums9dEfnUby1IiSYkKItjP2/4gM7+6ldSYIPx9zr5JcGZ8KGvSY/jaipSJvoQpb8wPMbXWe5RSrwIHABNwEHjSURUTQkysyuZuOvvMXDInho+L6tlV2oCXUpgtmqUpERgMiqyE0EEB3saq2dHn/FyDQfHcbRdMdPUF4xwHrrX+qdY6Q2s9T2v9Va11r6MqJoSYWAW2STp3rUklxN+b1w9WkVPWjFKwxDaNfX5iGAU1bdS29mBs7x3yAFM4n+zII8QUVVDThlIwPymMa+bH82ZuNfMSwkiPCyEswNp3PT8pjF6ThU221QLP9QBTTC6ZSi/EFFVY087MqCACfb25bnEiXX1m9p5sYumMU4tIzUu0Bvbf91UAkCktcJciAS7EFFVY20ZGvHVEyQUpkSSEWVfCyE45FeAzbQ8yjzd0khwZYG+ZC9cgAS7EFNTZa6KsqYuMadYWtcGg2Lg4EYDsGZH2cgaDsre6s+Kl+8TVSB+4EFPQsbp2tGbImO57181m9ezoYRsozE8MY++JJnmA6YKkBS7EFFRgG/89N/5UKAf6erNyhGGCC5KsLe+sRAlwVyMBLsQUVFjTTrCfN0kRAecse0XWNH76uUxWp8lSGK5GulCEmIIKa9vImBYyqiVc/X28uG3VzEmolThf0gIXYorRWlNY0z6k+0S4JwlwIVxUUV07Nz65m89KG8f9WS/tKeO+vx3E2NZDZXM37b0m+xBC4b6kC0UIF7SrpIE7X9xPe4+JurYe3r//4iEbI4yk32zhyR3HSY4M5FrbpgsAXX0mfv1uIW09JnYU1bNhgfXcwBBC4b6kBS6Ei3n9YCW3PLuXaaH+/Od18zje0MkLn5Wd9T11bT185andPPL+MX74Wh5NnX2DPq+Kth4Tv/7ifBIjAnhht/Wz0s+xLKxwfRLgQriQ4/Ud3P/3XJbOiODVb67k5gunszotmse3FtPS1Tfiez4taeCa/91JfnUbD1yRTne/mf/7uBSw9nc/9+lJ5iWG8uXsZP75zVXctz6NW1emEOwn/wB3dxLgQriQnDLrdma/uG4+YQE+KKX48TWZtPf089iHxUPKtnT18f1X87jp6T2EB/qy6e5V3L12NtctSuT5z05ibOvhk5IGio0d3LZyJkopfL0N3H/ZHB66NssZlyccTP4KFsKF5FW2EOznzazoIPux9Gkh3HjBdF7YXUZsqB8BPl5095t5ZucJWrr7ufPiWdx3aRqBvtb/ne+7NI1NudU8sb2U8qYuooP92LAw3lmXJCaQBLgQLuRwZSvzEkMxGIaOz/72ZXPYfqyeh987Zj+2KDmcFz4/f9gKgTOigvhydhIv7SnDZNF8a13asM2HhWeQABfCRfSZLBTUtHPbqpRh56KC/dj+wBq6+81YLBqzRRMZ5HvGiTj3rEvjtf1VeBvgpuXTJ7jmwlkkwIVwAq01lc3dQxaOOlbbTp/ZwvykkVf98/EynHMo4YDE8AB+dM1c+kwWYkP8HVJn4XrkIaYQTrD5cC2rH95GXmWL/Viu7eeFSeEO+Y5bVqbwbxfPcshnCdckAS6EE7x+0LpF2RsHq+3HDle2EhHoM6oFpoQACXAhJl1bTz87iuoB2Hy4BotFA9YW+Pyk8FEtMCUESIALMek+PFpHn9nCrStTqG3rIaesme4+M8XGDhYkyq43YvQkwIWYZJsP15AQ5s93r0jH38fA23nVHK1pxWzR9s0ThBgNCXAhJlC/2UKJsd3+urW7nx1FDVw1P55gP2/WZcSy+XANB8tbAFjgoAeYYmqQABdinCwWzbZCI9sKjcPO/f6jEi59dAf/yKkATnWfXLPAOjNyw4IEGjr6eG7XSWJD/JgWJkP+xOjJOHAhxqjPZGHToSqe3HGcYmMHft4GDv7kMvuUdoAPjtYB8P3X8gjy82bz4RoSwwNYnBwOwNr0WAJ9vahs7ubSubHOuAzhxqQFLsQYaK35xov7eeDVPLwMijsvmUWvycKOogZ7mdrWHgpq2vjW+jSWTI/gvr8d5OOieq6aN80+0iTA14tL58YB0n0izp8EuBBj8GGBkY8KjXz38jm8e99qHrg8nfBAHz7Ir7WX2XbM2qVyzfx4/nzbMtKnhWCyaHv3yYDrFls3WFg6I2LyLkB4BOlCEeI89ZrM/OKdo8yODebOS1JRSuHtpVifEceWo7X0my34eBn4qNBIYngAc+KCUUrx0u3LOVDRzOLpQ4N6bXosb997EVkJskOOOD/jaoErpcKVUq8qpQqVUgVKqRWOqpgQruqZT05Q1tjFTzZkDlmb5IqsONp6TOw53kSvycynJQ2szYixd5eEBfqwNn14P7dSinmJYTKBR5y38bbAHwfe01pfr5TyBQLP9QYh3FldW491ZMncOC6eEzPk3Oq0GPx9DLyfX4tG09VnHjGwhXCUMQe4UioMuBi4FUBr3QeMvOeTEB7il5sLMJk1D26YO+xcgK8Xl8yJ4YOjtXgZFH7eBlamRjuhlmKqGE8XykygHnhWKXVQKfW0Uiro9EJKqTuUUjlKqZz6+vpxfJ0QzvVWbjWbDlXzzTWpzIga9kcdgCuyplHX1ssrORWsSI0iwFc2UhATZzwB7g0sAf6otV4MdAI/OL2Q1vpJrXW21jo7Jibm9NNCuIWqlm5+9PphFiWHc8+62Wcstz4jDm+Dku4TMSnGE+CVQKXWeo/t9atYA10Ij2K2aL7990OYLZrHb1h01k0VwgJ9WD4rCoB1GRLgYmKNuQ9ca12rlKpQSqVrrY8B64GjjquaEM5nMlt4dEsRe0408T9fWnjGrpPB7l47m4XJYUN22xFiIox3FMq9wEu2ESjHgdvGXyUhnM9i0bx9uIbHthRxvKGT6xYl8MUliaN674rUKFakRk1wDYUYZ4BrrQ8B2Y6pihATr7vPzP1/P8R3r5jD7NiQM5a75dm97CxuID0uhP/76lIuz4yTcdrC5chMTDGl5JQ18V5+LdPC/Hno2qwRy5QYO9hZ3MDda1P5zmXpGAwS3MI1yVooYkrJq2wF4IP8WrTWI5b54Kh1PZObl8+Q8BYuTQJcTCkDu8BXt/aQX902YpkP8utYmBRGfJhsLixcmwS4mFLyKltZnRaNQTFk5cABdW09HKpo4fKsaU6onRDnRwJcTBnG9h5qWnu4ZE4M2SmR9s0WBttiO3Z5ZtxkV0+I8yYBLqaMw7b+74XJ4VyeGUdhbTtljZ1DynxwtI6Z0UHMjg12RhWFOC8S4MJjtHb1s+2Y8YwPJ3MrWzEoyEoI5QpbF8mWQa3wtp5+PittkCGDwm1IgAuPsLO4nise28Ftz+7jncM1I5bJq2whLTaEQF9vkiMDmRsfygf5pwJ8+7F6+s2ay7Ok+0S4Bwlw4dZ6+s089GY+X31mL0F+XqTGBPHr9wrpNZmHlNNak1fZyoKkMPuxyzPj2FfWRH17Lz39Zt4/Ukt0sB+LkmVrM+EeJMCFW/vLZyd5btdJbl2ZwjvfWs3Prp1HRVM3z+86OaRcVUs3TZ19LLDtBg9weVYcWsOy//qQjAff453DNVyWGYuXjP0WbkJmYgq39nFRPRnTQuyzKi9Ki2Ztegy/+6iE65cmExnkC5yawLMg8VQLPDM+lJ9vzKKxow9fbwP+Pl587rQNh4VwZRLgwi3sPt5IXmULd1ycaj/W029m38lmvrZ8xpCy/3H1XK58fCePf1jEzzbOAyC3sgUfL0VG/Kn1T5RSfG1FyqTUX4iJIF0owi08sb2UX24upLql235s38km+kwWVqUN3bYsLS6EG5Yl8+Kecv6RU2Ht/65oZW58KH7eskOO8BwS4MLl9Zks7DvRBMA7eadGmHxS0oCPl+LCmZHD3vPdy9NZOj2CB17N41+f28eRqlbmD+o+EcITSIALl5db2UJ3vxlfbwNv5lbbj39a0sCS6REE+g7vCYwI8uVvdyznoc9lsvt4E+29JhYmhU9irYWYeBLgwuXtKmlEKbhj9SwOV7VyoqGTps4+8qvbuGj2mXd9NxgUt66ayXv/vpq71qRy1XxZ30R4Fglw4fJ2lTaQlRDKVy6cDsDbudXsKm1Aa+uok3OZERXE967MIMTfZ6KrKsSkkgAXLq2n38zB8hZWpkaTEB7AspQI3sqr5tOSBkL8vaVfW0xpEuDCKe5+6QBP7ig9Z7n9Zc30mS32PSY/tzCBoroO3sqtYcWsKLzPskO8EJ5O/vSLSdfTb+bdIzX8+ZOTWCwjLzw1YFdpA94GxbIU60iTq+fHY1DQ0Wti9Si6T4TwZBLgYtIV1bVj0VDb1sP+8uazlt1V2sjC5HCC/awjTaKD/Vhle3C56iwPMIWYCiTAxaQrqLFuZWZQ1geSZ9Le009eZSsrZkUNOX7P2tncujKFmdFBE1pPIVydBLiYdAU17QT6enFZZhybj9RiPkM3yr6TTZgtmpWpQwP8wllRPHRtlqzZLaY8CXAx6Y7WtJE+LYRrFyZS397LnhONw8r0mSy8dqAKX28DS2bI8q5CjEQCXEwqrTWFNW3MjQ9lXUYsgb5evJ03dAOGg+XNbPjdTt7Jq+GWFTPw95H1S4QYiQS4mFTVrT209ZiYGx9KgK8X6+fG8d6RWkxmC209/Tz0Zj5f+OMu2ntM/PnWbH50TaazqyyEy5LlZMWkKqi2PsDMtC3rumFBPG/lVvPLzYW8lVdNQ0cvN104ne/LzEkhzkkCXEyqwlprgKdPCwXgkjkxBPt58+dPT7AgKYynv5bNwkG75gghzkwCXEyqgpp2pkcG2sd1+/t48esvLqCzz8QXlyTJdmZCnIdxB7hSygvIAaq01hvGXyXhSQpr25gZHWTfSKGgpo25g3bFAbhGtjETYkwc8RDzPqDAAZ8jPEi/2cJ/vn2UKx/byY9ePwJAV5+JE42dzI0PdXLthPAM4wpwpVQScA3wtGOqIzyBsa2Hm57awzOfnCA9LoRX91dyoLyZoroOtIaMaRLgQjjCeFvgjwHfAyxnKqCUukMplaOUyqmvrx/n1wlXV9bYyYbffcLhqlYev2ERr921ktgQPx56M5/8auvO8JnSAhfCIcYc4EqpDYBRa73/bOW01k9qrbO11tkxMTFj/TrhBlq7+/nX5/bRZ7bwz7tWsnFRIsF+3vzH1XPJq2zl9x+VEOznTVJEgLOrKoRHGE8LfBVwrVLqJPA3YJ1S6kWH1Eq4JLNFc6iiBa2Hr13Sb7Zw10v7KW/q4k83Lx3Sz71xUQLLUiKoae0hY1oIBhlpIoRDjDnAtdY/1Fonaa1TgBuAj7TWNzusZsLl/HZLEdf94VMe3HRkyDreFovmwTeO8GlJI7/6wgKWn7Z6oFKKh67NwqAgM0G6T4RwFBkHLkalsLaNP31cyvTIQF7cXU5Xr5mHr19ASX0HP379CDllzdy9NpXrlyaN+P6shDBe/rflzIyRJWCFcBSHBLjWejuw3RGfJVyP2aL5/muHCQvwYdPdq3hxdxm/2VJEkbGdwpp2Qvy9efiLC/hS9sjhPeDC01rmQojxkRb4FNNnsuBlUOc14/Evn50kt6KFx29YRESQL/euTyPA14tfvVvI9UuS+MFVGUQE+U5grYUQI5EAnwK01hwob+HV/RW8nVtDoJ8XT9y0hKUzIs/53qqWbh55/xhr0mO4dmGC/fjXV8/i5uWy1KsQziQB7qG2HTOy5WgdpcYOSus7aOjoI8DHi6vmTWN/eTM3PLmbBzdk8tXlM866s83vthZjsmh+cd28YeUkvIVwLglwD6S15ruv5NJrspA+LYR1GbFkz4jk6gXxBPt509rVz7dfOcRPNuWTc7KZn2/MIjxweBdITWs3rx2o5IZl00mKCHTClQghzkYC3ANVNnfT2NnHL66bx83LZww7Hxbow1Nfy+YP20p4fGsxu0ob+PnGeVw9f+iiUk/vPIFFwx0Xz5qsqgshzoPsyOOBDlW0ALDoLOtqGwyKe9ensemeVUwL8+eulw5w90sH6OozAdDc2cdf95SzcWECyZHS+hbCFUmAe6DcihZ8vQ2kTws5Z9mshDDeuGsVD1yRzrtHarjxyd3Ut/fy7K6TdPeb+caa1EmosRBiLKQLxQPlVrYwLyEUH6/R/f3s7WXg7rWzmRMXwr0vH+ALf/yUtm4Tl2XGMSfu3H8JCCGcQ1rgHsZktnC4qnVM25JdlhnH3+5YQVevmdbufu6S1rcQLk1a4B6mqK6Dnn7LWfu/z2ZRcjib7llFfnUbi6dHOLZyQgiHkgD3MHmVLQAsTAof82ckRQTKsEEh3IB0oXiY3MoWwgJ8mBElASyEp5MAd1Faaz4pbhiybOtoHKqw9n+fbXalEMIzSIC7qJyyZm5+Zg9v5VWP+j1dfSaK6tpZlBQ2gTUTQrgKCXAXVVjbDsCWo3Wjfk9+dRtmix7TCBQhhPuRAHdRpcYOAD4uqqfPdMY9o4fItc3AXDCOB5hCCPchAe6iSowdeBkU7T0m9p1sGtV7DlW0kBgeQEyI3wTXTgjhCiTAXVRpfQeXZ8bh623gw4Jzd6O0dvez90QTC5Ol/1uIqUIC3AmOVLXyw3/mYT7DCJOOXhM1rT3MSwzjotnRfFhQN+JO8AN6TWbufCGH5q4+blmRMkG1FkK4GglwJ3grt5qX91ZwzPag8nQD/d+zY4NZPzeWiqZuim3HTmexaL79Si67jzfxP19aKPtOCjGFSIA7wUAY59pmTZ6utN56PjUmmPUZccDIo1E6ek08uOkI7+TV8B9XZ7BxUeLEVFgI4ZIkwJ2gxBbgeWcI8BJjB94GxYyoQKaF+TM/MYytg/rBq1u6+dXmAlb8aisv7Snn6xfN5N9Wy6YLQkw1shbKJOvuM1PR3AVYZ02OpMTYQUp0kH052EvnxvHY1iL++91CPi1p4HBVK14GxVXzpnH7RTNl0SkhpigJ8ElWWt+B1pAaE0RRXTtdfSYCfb2HlZkdG2x/fXlWHL/9sIgnd5SyZHoE37lsDp9fkigLTgkxxUmAT7KB/u3rlybz6/cKya9uY1lKpP18v9lCWWMXV86bZj82Nz6Ud+9bTUJYAGGBPpNeZyGEa5I+8ElWXGedoHPd4gTg1OzJAWWNnZgsekgLHKwhLuEthBhMAnySFRvbmREVSHxYAInhAfYNiAeUGDsBmB0jW5kJIc5OAnySlRg7SLO1rhclhw8bSjjQxTIrJmiyqyaEcDMS4JOoz2ThZGMXabHW1vXC5DAqmrpp7Oi1lykxdpAQ5k+QnzyeEEKc3ZgDXCmVrJTappQ6qpTKV0rd58iKuaKndx7n46L6Mb//ZGMnZosmLc7aAh/Y9iyv8tRwwtL6DlJP6/8WQoiRjKcFbgK+o7XOBJYDdyulMh1TLddjtmgeef8YP3sr/6zrknxS3MBTO47T028edm5gAk9qjDWg5yWGYVDY+8G11pQaO+znhRDibMYc4FrrGq31AdvP7UAB4LFzuSubu+g1WThe38neEyMv76q15sFNR/ivzQVc/tsdbDtmHHK+uK4DpU4FeJCfN3PiQuz94DWtPXT2mYeNQBFCiJE4pA9cKZUCLAb2jHDuDqVUjlIqp75+7N0PzlZcd2oxqZf3lo9YJq+ylRMNndy8fDreXorbnt3Hd17JtbfYi43tJEcEEuDrZX/PwqRwDpa38N/vFnLbs/sA7A85hRDibMYd4EqpYOA14N+11m2nn9daP6m1ztZaZ8fExIz365ymxDY65POLE9l8pJbmzr5hZd44VIWvl4EHrsjg3ftWc+fFs3jtQCWvH6yyfoaxY1jremlKBK3d/Ty98zhhgT788KoMls6QqfFCiHMb11AHpZQP1vB+SWv9T8dUyTUV13UQG+LHnZfM4vWDVbx2oJKvD1pAymS28FZuDesyYgkLsE64+f6VGew92cQv3ilgdVoMxxs6uWTO0L/EPr84kZSoIObGhxDiLxN1hBCjN55RKAp4BijQWj/quCq5phJjO2lxwWRMC2Xx9HBe3ls+5GHmp6WNNHT02mdYAhgMil99YT5t3f3c89cD9Jksw1rgPl4GLpgZKeEthDhv4+lCWQV8FVinlDpk+3W1g+rlUrTWtgk41vHbN14wndL6TvadbLaX2XSwilB/b9akxw55b8a0UO68ZBZ7bA8+5QGlEMJRxjMK5ROttdJaL9BaL7L92uzIyrmK00eHbFgQT4ifNz9/O5/S+g66+8y8n1/L1fPj8ffxGvb+e9elkRJlXTlQAlwI4Sgy3W8UBnbQGRgdEujrzcPXL+D7r+Vx1eM7uWRODJ195jPuiOPv48UfblrC7uNN0lUihHAYCfBRKK6z7l05uPV81fx4lqZE8NCb+Ww+XEt8mD8Xzow800eQlRBGVoLsGC+EcBwJ8FEoMXYQGeRLVLDfkOOxIf48cdNSdhTVE+zvjcGgnFRDIcRUJAE+CsUjjN8e7OI57ju+XQjhvmQ1wnM4NQJFHj4KIVyLRwZ4T7+ZnJMjr1dyvuo7emnt7pfRI0IIl+ORAf7i7jKu/9Nn9s0RxqOkbmAEiuyQI4RwLR4Z4AOrBW4rNJ6j5Cn9ZsuIx+1DCOOkBS6EcC0eF+Baaw6UtwCw/djoVj88WN7M4p9v4ZWcimHnSowdhPh7ExviN8I7hRDCeTwuwCuaumno6CU62Je9J5ro7DWdtXxLVx/3/PUgHb0mHn6vkI7Tyhcb20mLDca69IsQQrgOjwvw/eXW7pNvrplNn9nCZ6WNZyyrtea7/8jF2N7Dz67NoqGjj6d2HLefb+/pp7C2XR5gCiFckucFeFkzwX7e3HThdIJ8vYbtijPY0ztP8GGBkR9eNZdbVqZwzfx4ntp5HGN7Dz39Zu74y346ekx8YUnSJF6BEEKMjsdN5DlQ1sKi5HD8fbxYNTua7cfq0VoP6wL58Ggdv36vkCuzpnHbqhQAHrginffza/ntliJauvr57Hgjv/2XhSyfFeWEKxFCiLPzqBZ4R6+Jwto2lth2tFmTHktVS7d9M+EB7x6u4Rsv7icrIZSHv7TAHu4p0UHcdOF0Xt5bwbtHavnxNXP5/GJpfQshXJNHBXhuRQsWjX1LsjXp1inug0ejbDpUxT0vH2RRcjgvfv1CQk9bHfDe9WkkRQRwz9rZQ3bcEUIIV+NRXSj7y5pRChYlhwOQEB5AelwI244ZyU6J4OlPTrD5cA0XzozkmVuWEeQ3/PKjg/3Y+b21MupECOHy3CLALRZNn9ky4mYJgx0ob2ZObIh9T0qwtsL/b8dxPv/ELkL8vbnj4ln8+/o5Q3aGP52EtxDCHbh8gGut+fYrh2jt7uepr2Xj7TVyr4/FojlQ1sw1C+KHHP9SdhIHy1u4ct40vrwsmeARWt1CCOGOXL4PXCnFBTOj2Hasngc35Q/ZSHiw0voO2npMLJkeMeT47NgQXvnGCv71opkS3kIIj+IWifaVC6dT2dzFE9tLSY4M4K41s4eVGdg0eOABphBCeDq3CHCA716eTmVzNw+/d4zE8IAh+0929pp4YlsJ6XEhzIwOcmIthRBi8rhNgBsMike+tIC6th4e+EcecaH+9gk2j24porq1h9e+slgeQAohpgyX7wMfzM/biye/mk1yZAB3vrCfEmMHhytbefbTE9x04XSWzjjzpsJCCOFp3CrAAcICfXjutgvw8VLc+uxevv9aHpFBfnzvygxnV00IISaV2wU4QHJkIE/fsoyGjl6O1rTx089lDhn7LYQQU4Hb9IGfblFyOH++ZRmHKlvYcNrYbyGEmArcNsABVs6OZuXsaGdXQwghnMItu1CEEEJIgAshhNsaV4Arpa5USh1TSpUopX7gqEoJIYQ4tzEHuFLKC/gDcBWQCdyolMp0VMWEEEKc3Xha4BcAJVrr41rrPuBvwEbHVEsIIcS5jCfAE4GKQa8rbceGUErdoZTKUUrl1NfXn35aCCHEGE34Q0yt9ZNa62ytdXZMTMxEf50QQkwZ4wnwKiB50Osk2zEhhBCTQJ1pg4RzvlEpb6AIWI81uPcBX9Fa55/lPfVA2Zi+EKKBhjG+151NxeueitcMU/O65ZpHZ4bWelgXxphnYmqtTUqpe4D3AS/gz2cLb9t7xtyHopTK0Vpnj/X97moqXvdUvGaYmtct1zw+45pKr7XeDGx2REWEEEKcH5mJKYQQbsqdAvxJZ1fASabidU/Fa4aped1yzeMw5oeYQgghnMudWuBCCCEGkQAXQgg35RYBPhVWPVRKJSultimljiql8pVS99mORyqltiilim2/Rzi7ro6mlPJSSh1USr1tez1TKbXHdr//rpTydXYdHU0pFa6UelUpVaiUKlBKrfD0e62Uut/2Z/uIUuplpZS/J95rpdSflVJGpdSRQcdGvLfK6n9t15+nlFpyPt/l8gE+hVY9NAHf0VpnAsuBu23X+QNgq9Y6Ddhqe+1p7gMKBr3+NfBbrfVsoBm43Sm1mliPA+9prTOAhViv32PvtVIqEfgWkK21nod17sgNeOa9fg648rRjZ7q3VwFptl93AH88ny9y+QBniqx6qLWu0VofsP3cjvV/6ESs1/q8rdjzwHVOqeAEUUolAdcAT9teK2Ad8KqtiCdecxhwMfAMgNa6T2vdgoffa6zzTgJss7gDgRo88F5rrXcATacdPtO93Qj8RVvtBsKVUqPe5NcdAnxUqx56EqVUCrAY2APEaa1rbKdqgThn1WuCPAZ8D7DYXkcBLVprk+21J97vmUA98Kyt6+hppVQQHnyvtdZVwP8A5ViDuxXYj+ff6wFnurfjyjd3CPApRSkVDLwG/LvWum3wOW0d8+kx4z6VUhsAo9Z6v7PrMsm8gSXAH7XWi4FOTusu8cB7HYG1tTkTSACCGN7NMCU48t66Q4BPmVUPlVI+WMP7Ja31P22H6wb+SWX73eis+k2AVcC1SqmTWLvG1mHtGw63/TMbPPN+VwKVWus9ttevYg10T77XlwIntNb1Wut+4J9Y77+n3+sBZ7q348o3dwjwfUCa7Wm1L9YHH286uU4OZ+v7fQYo0Fo/OujUm8Attp9vATZNdt0mitb6h1rrJK11Ctb7+pHW+iZgG3C9rZhHXTOA1roWqFBKpdsOrQeO4sH3GmvXyXKlVKDtz/rANXv0vR7kTPf2TeBrttEoy4HWQV0t56a1dvlfwNVYl64tBX7k7PpM0DVehPWfVXnAIduvq7H2CW8FioEPgUhn13WCrn8N8Lbt51nAXqAE+Afg5+z6TcD1LgJybPf7DSDC0+818DOgEDgCvAD4eeK9Bl7G2s/fj/VfW7ef6d4CCusou1LgMNZROqP+LplKL4QQbsodulCEEEKMQAJcCCHclAS4EEK4KQlwIYRwUxLgQgjhpiTAhRDCTUmACyGEm/p/d08wzNcdIvUAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fake_x = np.arange(0,100,1)\n",
    "fake_y = np.arange(0,10,0.1) + np.random.rand(100)\n",
    "plt.plot(fake_x,fake_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Here, we won't split the data in train/val/test, this is just an example\n",
    "\n",
    "So, the linear model we want to learn is the following:\n",
    "$$f(x) =  wx+b $$\n",
    "The parameters to optimize are w and b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we create to tensor variables (which are our parameters)\n",
    "w = torch.tensor([1.],requires_grad=True) # We need to set requires_grad to True so the gradient can flow.\n",
    "b = torch.tensor([0.5],requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the f function:\n",
    "def f(x):\n",
    "    return (w*x)+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define an error function (here the MAE)\n",
    "def error(pred,real):\n",
    "    return (pred-real).abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "loss: 32.47551399290562\n",
      "w: 0.5049999952316284\n",
      "b: 0.49020129442214966\n",
      "----\n",
      "loss: 8.387166615724563\n",
      "w: 0.10500003397464752\n",
      "b: 0.4814024567604065\n",
      "----\n",
      "loss: 0.31570376694202423\n",
      "w: 0.10500003397464752\n",
      "b: 0.48060256242752075\n",
      "----\n",
      "loss: 0.3156397706270218\n",
      "w: 0.10500003397464752\n",
      "b: 0.479802668094635\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x26e291e2460>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Finally, we cycle through the data, optimizing the parameters with respect to the gradient of the error:\n",
    "plt.plot(fake_y)\n",
    "\n",
    "for epoch in range(4): # We cycle 4 times\n",
    "    mean_loss = 0\n",
    "    \n",
    "    for x,y in zip(fake_x,fake_y): \n",
    "        \n",
    "        pred = f(x) #predict\n",
    "        loss = error(pred,y) #compute loss\n",
    "        loss.backward() # This does backpropagation and sets .grad attribute.\n",
    "\n",
    "        # Update parameters via SGD:\n",
    "        with torch.no_grad(): # This deactivated gradient calculations\n",
    "            \n",
    "            mean_loss += loss.item() # get the raw value of a (1,) tensor\n",
    "            w -= 0.0001 * w.grad # This wouldn't be possible w/ gradient (-= is an inplace operation)\n",
    "            b -= 0.0001 * b.grad\n",
    "            w.grad.zero_()\n",
    "            b.grad.zero_()\n",
    "            \n",
    "    # Plot the resulting line        \n",
    "    predictions = [f(x).detach().numpy() for x in range(100)]\n",
    "    plt.plot(predictions,label=f\"epoch {epoch}\")\n",
    "\n",
    "    print('----')\n",
    "    print(\"loss:\", mean_loss/len(fake_y))\n",
    "    print('w:',w.item())\n",
    "    print('b:',b.item())\n",
    "    \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Full pytorch tutorial: \n",
    "\n",
    "A tutorial can be found [here](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) do not hesitate to take a couple of minutes to skim read it. Plenty of [ressources](https://pytorch.org/resources) are available online. Also, you can have a look at the [extensive pytorch documentation](https://pytorch.org/docs/stable/index.html). \n",
    "\n",
    "Here, as we are defining neural networks, we mainly use the `torch.nn` module which contains most classical deep learning building blocks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data used : [smallest movie-lens dataset](https://grouplens.org/datasets/movielens/)\n",
    "\n",
    "=> Just like the previous sessions\n",
    "\n",
    "\n",
    "# 1)  Load & Prepare Data\n",
    "\n",
    "To be able to embed the data easily, we need to remap  the user/items between [0->N_User] and [0->N_Items]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " #train:64535, #val:16133 ,#test:20168\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "## Load\n",
    "ratings = pd.read_csv(\"dataset/ratings.csv\")\n",
    "ratings.head(5)\n",
    "\n",
    "## Prepare Data\n",
    "user_map = {user:num for num,user in enumerate(ratings[\"userId\"].unique())}\n",
    "item_map = {item:num for num,item in enumerate(ratings[\"movieId\"].unique())}\n",
    "\n",
    "## Number of users & items\n",
    "num_users = len(user_map)\n",
    "num_items = len(item_map)\n",
    "\n",
    "ratings[\"userId\"] = ratings[\"userId\"].map(user_map)\n",
    "ratings[\"movieId\"] = ratings[\"movieId\"].map(item_map)\n",
    "\n",
    "ratings.head(5)\n",
    "\n",
    "# Creating Test/Train as before\n",
    "\n",
    "train_indexes,val_indexes,test_indexes = [],[],[]\n",
    "\n",
    "for index in range(len(ratings)):\n",
    "    if index%5 == 0:\n",
    "        test_indexes.append(index)\n",
    "    else:\n",
    "        train_indexes.append(index)\n",
    "\n",
    "        \n",
    "shuffle(train_indexes)\n",
    "num_val = int(len(train_indexes)/100*20)\n",
    "val_indexes = train_indexes[:num_val]\n",
    "train_indexes = train_indexes[num_val:]\n",
    "\n",
    "train_ratings = ratings.iloc[train_indexes].copy()\n",
    "val_ratings = ratings.iloc[val_indexes].copy()\n",
    "test_ratings = ratings.iloc[test_indexes].copy()\n",
    "\n",
    "\n",
    "print(f\" #train:{len(train_ratings)}, #val:{len(val_ratings)} ,#test:{len(test_ratings)}\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) Reproduce the baseline model with pytorch's vanilla autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your goal now is to reproduce the following baseline model from surprise\n",
    "\n",
    "## $$\\hat{r}_{ui} = b_{ui} = \\mu + b_u + b_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) : First, let's define the parameters\n",
    "\n",
    "You have many parameters, they are all 1-dimensional:\n",
    "- **mu:** the global mean (1,)\n",
    "- **bu:** the user means (n_users,)\n",
    "- **bi:** the item means (n_items,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = torch.tensor([ratings[\"rating\"].mean()], requires_grad=True)\n",
    "bu = [torch.tensor([x], requires_grad=True) for x in ratings.groupby(\"userId\")[\"rating\"].mean()]\n",
    "bi = [torch.tensor([x], requires_grad=True) for x in ratings.groupby(\"movieId\")[\"rating\"].mean()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Then, we define two functions: \n",
    "\n",
    "- `predict(u,i)` : Will return the prediction given the (user,item) pair\n",
    "- `error(pred,real)` : Will return the MSE error of prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (TODO) Predict Function\n",
    "This function should implement this: $\\hat{r}_{ui} = b_{ui} = \\mu + b_u + b_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(u,i):\n",
    "    # User mean\n",
    "    if u < num_users: # if user exist:\n",
    "        user_mean = bu[u]\n",
    "    else:\n",
    "        user_mean = 0\n",
    "\n",
    "    # Item mean \n",
    "    if i < num_items: # if item exist:\n",
    "        item_mean = bi[i]\n",
    "    else:\n",
    "        item_mean = 0\n",
    "        \n",
    "    # Like in the formula shown above, we return : mu + bu + bi\n",
    "    return mu + user_mean + item_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (TODO) error function\n",
    "We want to use the MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(pred,real):\n",
    "    # We return the squared difference between the truth and the prediction values\n",
    "    return (real - pred)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The evaluation loop, without any optimization for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final train error :  49.7796137707048\n",
      "final val error :  49.473817058557714\n",
      "final test error :  49.72078917672815\n"
     ]
    }
   ],
   "source": [
    "train_e = 0\n",
    "for index, uid, mid, r, ts in train_ratings.itertuples():\n",
    "    result = predict(uid,mid)\n",
    "    train_e += error(result,r).item()\n",
    "    \n",
    "val_e = 0\n",
    "for index, uid, mid, r, ts in val_ratings.itertuples():\n",
    "    result = predict(uid,mid)\n",
    "    val_e += error(result,r).item()\n",
    "\n",
    "test_e = 0\n",
    "for index, uid, mid, r, ts in test_ratings.itertuples():\n",
    "    result = predict(uid,mid)\n",
    "    test_e += error(result,r).item()\n",
    "\n",
    "print(\"final train error : \", train_e/len(train_ratings))\n",
    "print(\"final val error : \", val_e/len(val_ratings))\n",
    "print(\"final test error : \", test_e/len(test_ratings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's optimize the parameters (with SGD)  by slightly modifying the previous loop\n",
    "\n",
    "### (TODO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 train error :  49.7796137707048\n",
      "epoch 0 val error :  0.6526928174994974\n",
      "epoch 0 test error :  0.6411786294399203\n",
      "-----\n",
      "epoch 1 train error :  49.7796137707048\n",
      "epoch 1 val error :  0.6546947233048782\n",
      "epoch 1 test error :  0.652621880003589\n",
      "-----\n",
      "epoch 2 train error :  49.7796137707048\n",
      "epoch 2 val error :  0.6926512043939949\n",
      "epoch 2 test error :  0.6784380745035686\n",
      "-----\n",
      "epoch 3 train error :  49.7796137707048\n",
      "epoch 3 val error :  0.6652412904852445\n",
      "epoch 3 test error :  0.6637780392521719\n",
      "-----\n",
      "epoch 4 train error :  49.7796137707048\n",
      "epoch 4 val error :  0.6745200298061934\n",
      "epoch 4 test error :  0.6724228898239578\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01\n",
    "batch_size = 32\n",
    "n_epochs = 5\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    train_error = 0\n",
    "    \n",
    "    for num,(index, uid, mid, r, ts) in enumerate(train_ratings.sample(frac=1).itertuples()):\n",
    "        # uid = user id\n",
    "        # mid = movie id\n",
    "        # We use the functions we have coded before\n",
    "        result = predict(uid, mid)\n",
    "        ex_error = error(result, r)\n",
    "        train_error += ex_error.item()\n",
    "        \n",
    "        ex_error.backward()\n",
    "\n",
    "        if num % batch_size == 0:\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                mu -= lr * mu.grad\n",
    "                bu[uid] -= lr * bu[uid].grad\n",
    "                bi[mid] -= lr * bi[mid].grad\n",
    "\n",
    "                # Manually zero the gradients after updating weights\n",
    "                mu.grad.zero_()\n",
    "                bu[uid].grad.zero_()\n",
    "                bi[mid].grad.zero_()\n",
    "\n",
    "\n",
    "    print(f\"epoch {epoch} train error : \", train_e/len(train_ratings))\n",
    "    \n",
    "    val_e = 0\n",
    "    for index, uid, mid, r, ts in val_ratings.itertuples():\n",
    "        result = predict(uid,mid)\n",
    "        val_e += error(result,r).item()\n",
    "\n",
    "    print(f\"epoch {epoch} val error : \", val_e/len(val_ratings))\n",
    "\n",
    "\n",
    "    test_e = 0\n",
    "    for index, uid, mid, r, ts in test_ratings.itertuples():\n",
    "        result = predict(uid,mid)\n",
    "        test_e += error(result,r).item()\n",
    "\n",
    "    print(f\"epoch {epoch} test error : \", test_e/len(test_ratings))\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Pytorch (.nn) Modules\n",
    "\n",
    "Instead of having to define everything by hand, pytorch has several usefull abstractions:\n",
    "\n",
    "- `nn.Module()` -> To define the model and the forward computation\n",
    "- `torch.utils.data.DataLoader` -> To create the data pipeline\n",
    "\n",
    "To explore these modules, we'll do the following model:\n",
    "\n",
    "##  Classic SVD (with mean)\n",
    "\n",
    "To see how it works, we propose to implement a simple SVD:\n",
    "### $$ \\min\\limits_{U,I}\\sum\\limits_{(u,i)} \\underbrace{(r_{ui} -  (I_i^TU_u + \\mu))^2}_\\text{minimization} + \\underbrace{\\lambda(||U_u||^2+||I_u||^2 + \\mu) }_\\text{regularization} $$\n",
    "\n",
    "where prediction is done in the following way:\n",
    "### $$r_{ui} = \\mu + U_u.I_i $$\n",
    "\n",
    "where $\\mu$ is the global mean,  $U_u$ a user embedding and $I_i$ an item embedding\n",
    "\n",
    "### STEPS:\n",
    " To implement such model in pytorch, we need to do multiple things:\n",
    " \n",
    " - (1) model definition\n",
    " - (2) loss function\n",
    " - (3) evaluation\n",
    " - (4) training/eval loop\n",
    "\n",
    "\n",
    "#### (1) Model definition\n",
    "\n",
    "A model class typically extends `nn.Module`, the Neural network module. It is a convenient way of encapsulating parameters, with helpers for moving them to GPU, exporting, loading, etc.\n",
    "\n",
    "One should define two functions: `__init__` and `forward`.\n",
    "\n",
    "- `__init__` is used to initialize the model parameters\n",
    "- `forward` is the net transformation from input to output. In fact, when doing `moduleClass(input)` you call this method.\n",
    "\n",
    "##### (a) Initialization\n",
    "\n",
    "Our model has different weigths:\n",
    "\n",
    "- the user profiles (also called user embeddings) $U$\n",
    "- the item profiles (also called user embeddings) $I$\n",
    "- the mean bias $\\mu$\n",
    "\n",
    "\n",
    "##### (b) input to output operation\n",
    "Technically, the prediction as defined earlier can be seen as just a dot product between two embeddings $U_u$ and $I_i$ plus the mean rating:\n",
    "\n",
    "- `torch.sum(embed_u*embed_i,1) + self.mean` is equivalent to $r_{ui} = \\mu + U_u.I_i $ \n",
    "- the `.squeeze(1)` operation is a shape operation to remove the dimension 1 (indexing starts at 0) akin to reshaping the matrix from `(batch_size,1,latent_size)` to `(batch_size,latent_size)`\n",
    "- for reference, the inverse operation is `.unsqueeze()`\n",
    "- we return weights to regularize them\n",
    "\n",
    "\n",
    "### (TODO) Just to make sure you were following: complete the following `forward` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Let's create the datasets following  (Object w/ __getitem__(index) and __len()__, i.e lists ;)\n",
    "prep_train = [(tp.userId,tp.movieId,tp.rating) for tp in train_ratings.itertuples()]\n",
    "prep_val = [(tp.userId,tp.movieId,tp.rating) for tp in val_ratings.itertuples()]\n",
    "prep_test = [(tp.userId,tp.movieId,tp.rating) for tp in test_ratings.itertuples()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# The model define as a class, inheriting from nn.Module\n",
    "class ClassicMF(nn.Module):\n",
    "    \n",
    "    #(a) Init\n",
    "    def __init__(self,nb_users,nb_items,latent_size):\n",
    "        super(ClassicMF, self).__init__()\n",
    "        \n",
    "        #Embedding layers\n",
    "        self.users = nn.Embedding(nb_users, latent_size)        \n",
    "        self.items = nn.Embedding(nb_items, latent_size)\n",
    "        #The mean bias\n",
    "        self.mean = nn.Parameter(torch.FloatTensor(1,).fill_(3))\n",
    "        \n",
    "        #initialize weights with very small values\n",
    "        nn.init.normal_(self.users.weight,0,0.01)\n",
    "        nn.init.normal_(self.items.weight,0,0.01)\n",
    "\n",
    "    \n",
    "    # (b) How we compute the prediction (from input to output)\n",
    "    def forward(self, user, item): ## method called when doing ClassicMF(user,item)\n",
    "        \n",
    "        embed_u,embed_i = self.users(user).squeeze(1),self.items(item).squeeze(1)\n",
    "        out =  torch.sum(embed_u * embed_i, 1) + self.mean\n",
    "\n",
    "        return out, embed_u, embed_i, self.mean  # We return prediction + weights to regularize them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2-4) full train loop\n",
    "\n",
    "The train loop is organized around the [Dataloader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) class which Combines a dataset and a sampler, and provides single- or multi-process iterators over the dataset.\n",
    "\n",
    "We just redefine a collate function\n",
    "\n",
    "> collate_fn (callable, optional) – merges a list of samples to form a mini-batch.\n",
    "\n",
    "\n",
    "**NOTE:** The dataset argument can be a list instead of a \"Dataset\" instance (works by duck typing)\n",
    "    \n",
    "\n",
    "##### The train loop sequence is the following:\n",
    "    \n",
    "[Dataset ==Dataloader==> Batch (not prepared) ==collate_fn==> Batch (prepared) ==Model.forward==> Prediction =loss_fn=> loss <-> truth \n",
    "\n",
    "1] PREDICT\n",
    "- (a) The dataloader samples training exemples from the dataset (which is a list)\n",
    "- (b) The collate_fn prepares the minibatch of training exemples\n",
    "- (c) The prediction is made by feeding the minibatch in the model\n",
    "- (d) The loss is computed on the prediction via a loss function\n",
    "\n",
    "2] OPTIMIZE\n",
    "- (e) Gradients are computed by automatic backard propagation\n",
    "- (f) Parameters are updated using computed gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "epoch 0 mse (train/val/test) 1.049 / 0.896 / 0.894\n",
      "-------------------------\n",
      "epoch 1 mse (train/val/test) 0.774 / 0.8 / 0.791\n",
      "-------------------------\n",
      "epoch 2 mse (train/val/test) 0.6 / 0.783 / 0.775\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# HyperParameters\n",
    "n_epochs = 3\n",
    "batch_size = 16\n",
    "num_feat = 25\n",
    "lr = 0.01\n",
    "reg = 0.001\n",
    "\n",
    "\n",
    "#(b) Collate function => Creates tensor batches to feed model during training\n",
    "# It can be removed if data is already tensors (torch or numpy ;)\n",
    "def tuple_batch(l):\n",
    "    '''\n",
    "    input l: list of (user,item,rating tuples)\n",
    "    output: formatted batches (in torch tensors)\n",
    "\n",
    "    takes n-tuples and create batch\n",
    "    text -> seq word #id\n",
    "    '''\n",
    "    users, items, ratings = zip(*l) \n",
    "    users_t = torch.LongTensor(users)\n",
    "    items_t = torch.LongTensor(items)\n",
    "    ratings_t = torch.FloatTensor(ratings)\n",
    "    \n",
    "    return users_t, items_t, ratings_t\n",
    "    \n",
    "\n",
    "\n",
    "#(d) Loss function => Combines MSE and L2\n",
    "def loss_func(pred,ratings_t,reg,*params):\n",
    "    '''\n",
    "    mse loss combined with l2 regularization.\n",
    "    params assumed 2-dimension\n",
    "    '''\n",
    "    mse = F.mse_loss(pred,ratings_t,reduction='sum')\n",
    "    l2 = 0\n",
    "    for p in params:\n",
    "        l2 += torch.mean(p.norm(2,-1))\n",
    "        \n",
    "    return (mse/pred.size(0)) + reg*l2 , mse\n",
    "    \n",
    "#\n",
    "# Training script starts here\n",
    "#    \n",
    "\n",
    "\n",
    "model = ClassicMF(num_users,num_items,num_feat)\n",
    "\n",
    "\n",
    "\n",
    "# (a) dataloader will sample data from datasets using collate_fn tuple_batch\n",
    "dataloader_train = DataLoader(prep_train, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=tuple_batch)\n",
    "dataloader_val = DataLoader(prep_val, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=tuple_batch)\n",
    "dataloader_test = DataLoader(prep_test, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=tuple_batch)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Train loop\n",
    "for e in range(n_epochs):\n",
    "    mean_loss = [0,0,0] #train/val/test\n",
    "\n",
    "    ## Training loss (the one we train with)\n",
    "    \n",
    "    for users_t,items_t,ratings_t in dataloader_train:\n",
    "        model.train() # set the model on train mode\n",
    "        model.zero_grad() # reset gradients\n",
    "        \n",
    "        #(c) predictions are made by the model\n",
    "        pred,*params = model(users_t,items_t)\n",
    "        \n",
    "        #(d) loss computed on predictions, we added regularization\n",
    "        loss,mse_loss = loss_func(pred,ratings_t,reg,*params)\n",
    "        \n",
    "        loss.backward() #(e) backpropagating to get gradients\n",
    "        \n",
    "        mean_loss[0] += mse_loss\n",
    "        optimizer.step() #(f) updating parameters\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        ## Validation loss (no training)\n",
    "        for users_t,items_t,ratings_t in dataloader_val:\n",
    "\n",
    "            model.eval() # Inference mode\n",
    "            pred,*params = model(users_t,items_t)\n",
    "            _,mse_loss = loss_func(pred,ratings_t,reg,*params)\n",
    "\n",
    "            mean_loss[1] += mse_loss    \n",
    "\n",
    "        ## Test loss (no training)\n",
    "\n",
    "        for users_t,items_t,ratings_t in dataloader_test:\n",
    "            model.eval()\n",
    "            pred,*params = model(users_t,items_t)\n",
    "            _,mse_loss = loss_func(pred,ratings_t,reg,*params)\n",
    "\n",
    "            mean_loss[2] += mse_loss    \n",
    "\n",
    "    print(\"-\"*25)\n",
    "    print(\"epoch\",e, \"mse (train/val/test)\", round((mean_loss[0]/len(prep_train)).item(),3),\"/\",  round((mean_loss[1]/len(prep_val)).item(),3),\"/\",  round((mean_loss[2]/len(prep_test)).item(),3))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Your turn) Koren 2009 model:\n",
    "\n",
    "Here, this model simply adds a bias for each user and for each item\n",
    "\n",
    "### $$ \\min\\limits_{U,I}\\sum\\limits_{(u,i)} \\underbrace{(r_{ui} -  (I_i^TU_u + \\mu+ \\mu_i+\\mu_u))^2}_\\text{minimization} + \\underbrace{\\lambda(||U_u||^2+||I_u||^2 + \\mu  + \\mu+ \\mu_i+\\mu_u) }_\\text{regularization} $$\n",
    "\n",
    "\n",
    "### $$r_{ui} = \\mu + \\mu_i + \\mu_u + U_u.I_i $$\n",
    "\n",
    "### TODO:\n",
    "\n",
    "- (a) complete the model initialization\n",
    "- (b) complete the forward method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KorenMF(nn.Module):\n",
    "\n",
    "    def __init__(self,nb_users,nb_items,latent_size):\n",
    "        super(KorenMF, self).__init__()\n",
    "        \n",
    "        self.users = nn.Embedding(nb_users, latent_size)\n",
    "        self.items = nn.Embedding(nb_items, latent_size)\n",
    "        self.umean = nn.Embedding(nb_users, 1)\n",
    "        self.imean = nn.Embedding(nb_items, 1)\n",
    "        self.gmean = nn.Parameter(torch.FloatTensor(1,).fill_(3))\n",
    "\n",
    "        nn.init.normal_(self.users.weight,0,0.01)\n",
    "        nn.init.normal_(self.items.weight,0,0.01)\n",
    "        nn.init.normal_(self.umean.weight,2,1)\n",
    "        nn.init.normal_(self.imean.weight,2,1)\n",
    "        \n",
    "        \n",
    "    def forward(self, user,item):\n",
    "        embed_u,embed_i = self.users(user).squeeze(1) , self.items(item).squeeze(1)\n",
    "        umean, imean = self.umean(user) , self.imean(item)\n",
    "        # We return the same formula as shown above\n",
    "        out = self.gmean + umean.view(-1) + imean.view(-1) + torch.sum(embed_u * embed_i, 1)\n",
    "\n",
    "        return out , embed_u, embed_i, umean , imean , self.gmean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (TODO) Here, train loop stays the same, you only have to change the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "epoch 0 mse (train/val/test) 5.833 / 1.858 / 1.895\n",
      "-------------------------\n",
      "epoch 1 mse (train/val/test) 1.423 / 1.248 / 1.274\n",
      "-------------------------\n",
      "epoch 2 mse (train/val/test) 0.979 / 1.018 / 1.029\n",
      "-------------------------\n",
      "epoch 3 mse (train/val/test) 0.785 / 0.935 / 0.941\n",
      "-------------------------\n",
      "epoch 4 mse (train/val/test) 0.681 / 0.898 / 0.905\n",
      "-------------------------\n",
      "epoch 5 mse (train/val/test) 0.597 / 0.885 / 0.89\n",
      "-------------------------\n",
      "epoch 6 mse (train/val/test) 0.515 / 0.883 / 0.892\n",
      "-------------------------\n",
      "epoch 7 mse (train/val/test) 0.436 / 0.889 / 0.891\n",
      "-------------------------\n",
      "epoch 8 mse (train/val/test) 0.364 / 0.91 / 0.91\n",
      "-------------------------\n",
      "epoch 9 mse (train/val/test) 0.303 / 0.937 / 0.934\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 16\n",
    "num_feat = 25\n",
    "lr = 0.01\n",
    "reg = 0.001\n",
    "\n",
    "\n",
    "\n",
    "def tuple_batch(l):\n",
    "    '''\n",
    "    input l: list of (user,item,review, rating tuples)\n",
    "    output: formatted batches (in torch tensors)\n",
    "\n",
    "    takes n-tuples and create batch\n",
    "    text -> seq word #id\n",
    "    '''\n",
    "    users, items,ratings = zip(*l)\n",
    "    users_t = torch.LongTensor(users)\n",
    "    items_t = torch.LongTensor(items)\n",
    "    ratings_t = torch.FloatTensor(ratings)\n",
    "    \n",
    "    return users_t,items_t,ratings_t\n",
    "\n",
    "\n",
    "def loss_func(pred,ratings_t,reg,*params):\n",
    "    '''\n",
    "    mse loss combined with l2 regularization.\n",
    "    params assumed 2-dimension\n",
    "    '''\n",
    "    mse = F.mse_loss(pred,ratings_t,reduction=\"sum\")\n",
    "    l2 = 0\n",
    "    for p in params:\n",
    "        l2 += torch.mean(p.norm(2,-1))\n",
    "        \n",
    "    return (mse/pred.size(0)) + reg*l2 , mse\n",
    "    \n",
    "# We use the class initialized in the previous cell\n",
    "model = KorenMF(num_users, num_items, num_feat)\n",
    "\n",
    "\n",
    "dataloader_train = DataLoader(prep_train, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=tuple_batch)\n",
    "dataloader_val = DataLoader(prep_val, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=tuple_batch)\n",
    "dataloader_test = DataLoader(prep_test, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=tuple_batch)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "for e in range(n_epochs):\n",
    "    mean_loss = [0,0,0] #train/val/test\n",
    "\n",
    "    for users_t,items_t,ratings_t in dataloader_train:\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        pred,*params = model(users_t,items_t)\n",
    "\n",
    "        loss,mse_loss = loss_func(pred,ratings_t,reg,*params)\n",
    "        loss.backward()\n",
    "        \n",
    "        mean_loss[0] += mse_loss\n",
    "        optimizer.step()\n",
    "    \n",
    "    \n",
    "\n",
    "    for users_t,items_t,ratings_t in dataloader_val:\n",
    "        model.eval()\n",
    "        pred,*params = model(users_t,items_t)\n",
    "        _,mse_loss = loss_func(pred,ratings_t,reg,*params)\n",
    "    \n",
    "        mean_loss[1] += mse_loss    \n",
    "        \n",
    "    for users_t,items_t,ratings_t in dataloader_test:\n",
    "        model.eval()\n",
    "        pred,*params = model(users_t,items_t)\n",
    "        _,mse_loss = loss_func(pred,ratings_t,reg,*params)\n",
    "    \n",
    "        mean_loss[2] += mse_loss    \n",
    "\n",
    "    print(\"-\"*25)\n",
    "    print(\"epoch\",e, \"mse (train/val/test)\", round((mean_loss[0]/len(prep_train)).item(),3),\"/\",  round((mean_loss[1]/len(prep_val)).item(),3),\"/\",  round((mean_loss[2]/len(prep_test)).item(),3))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch's keras: Pytorch-Lightning\n",
    "\n",
    "Pytorch lightning is a easy to use framework for Pytorch. To start a new project you just need to define two files:\n",
    "\n",
    "- a LightningModule (which inherits `pl.LightningModule`)\n",
    "- a Trainer file. \n",
    "\n",
    "By defining those two files, you get:\n",
    "- Checkpointing\n",
    "- Debugging\n",
    "- Distributed training\n",
    "- Experiment Logging\n",
    "- Training loop\n",
    "- Validation loop\n",
    "- Testing loop\n",
    "\n",
    "## Let's try with the same but different Koren 2009 model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### $$r_{ui} = \\mu + \\mu_i + \\mu_u + U_u.I_i $$\n",
    "\n",
    "Where the goal is to minimize the following loss\n",
    "\n",
    "### $$ \\min\\limits_{U,I}\\sum\\limits_{(u,i)} \\underbrace{(r_{ui} -  (I_i^TU_u + \\mu+ \\mu_i+\\mu_u))^2}_\\text{minimization} + \\underbrace{\\lambda(||U_u||^2+||I_u||^2 + \\mu  + \\mu+ \\mu_i+\\mu_u) }_\\text{regularization} $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) Complete the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class LightningKorenMF(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,nb_users,nb_items,latent_size):\n",
    "        super(LightningKorenMF, self).__init__()\n",
    "        \n",
    "        self.reg = 0.001\n",
    "        \n",
    "        self.users = nn.Embedding(nb_users, latent_size)\n",
    "        self.items = nn.Embedding(nb_items, latent_size)\n",
    "        self.umean = nn.Embedding(nb_users, 1)\n",
    "        self.imean = nn.Embedding(nb_items, 1)\n",
    "        self.gmean = nn.Parameter(torch.FloatTensor(1,).fill_(3))\n",
    "\n",
    "        nn.init.normal_(self.users.weight,0,0.01)\n",
    "        nn.init.normal_(self.items.weight,0,0.01)\n",
    "        nn.init.normal_(self.umean.weight,0.1,0.1)\n",
    "        nn.init.normal_(self.imean.weight,0.1,0.1)\n",
    "        \n",
    "\n",
    "    def forward(self, user,item):\n",
    "        embed_u,embed_i = self.users(user).squeeze(1) , self.items(item).squeeze(1)\n",
    "        umean, imean = self.umean(user) , self.imean(item)\n",
    "        out = self.gmean + umean.view(-1) + imean.view(-1) + torch.sum(embed_u * embed_i, 1)\n",
    "\n",
    "        return out , embed_u, embed_i, umean , imean , self.gmean\n",
    "\n",
    "    \n",
    "    def my_loss_func(self, pred,ratings_t,reg,*params):\n",
    "        '''\n",
    "        mse loss combined with l2 regularization.\n",
    "        params assumed 2-dimension\n",
    "        '''        \n",
    "        mse = F.mse_loss(pred,ratings_t)\n",
    "        l2 = 0\n",
    "        for p in params:\n",
    "            l2 += torch.mean(p.norm(2,-1))\n",
    "\n",
    "        return mse + reg*l2 , mse\n",
    "    \n",
    "    def training_step(self, batch, batch_nb):\n",
    "        # REQUIRE\n",
    "        users_t,items_t,ratings_t = batch\n",
    "        pred , *params = self.forward(users_t,items_t) \n",
    "        loss,mse = self.my_loss_func(pred,ratings_t,self.reg,*params)\n",
    "\n",
    "        return {'loss':loss,\"mse\":mse}\n",
    "    \n",
    "\n",
    "    \n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        return {\"val_mse\":self.training_step(batch,batch_nb)[\"mse\"]}\n",
    "    \n",
    "    def validation_end(self,outputs):\n",
    "        return {\"progress_bar\":{\"val_mse\":torch.tensor([output['val_mse'] for output in outputs]).mean().item()}}\n",
    "    \n",
    "    def test_step(self, batch, batch_nb):\n",
    "        return {\"test_mse\":self.training_step(batch,batch_nb)[\"mse\"]}\n",
    "    \n",
    "    def test_end(self,outputs):\n",
    "        res = {\"progress_bar\":{\"test_mse\":torch.tensor([output['test_mse'] for output in outputs]).mean().item()}}\n",
    "        print(res)\n",
    "        return res\n",
    "\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # REQUIRED\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.002)\n",
    "    \n",
    "    def tuple_batch(self,l):\n",
    "        '''\n",
    "        input l: list of (user,item,rating tuples)\n",
    "        output: formatted batches (in torch tensors)\n",
    "\n",
    "        takes n-tuples and create batch\n",
    "        text -> seq word #id\n",
    "        '''\n",
    "        users, items, ratings = zip(*l) \n",
    "        users_t = torch.LongTensor(users)\n",
    "        items_t = torch.LongTensor(items)\n",
    "        ratings_t = torch.FloatTensor(ratings)\n",
    "\n",
    "        return users_t, items_t, ratings_t\n",
    "    \n",
    "    # Note - The pl.data_loader decorator is no longer used\n",
    "    # https://forums.pytorchlightning.ai/t/attributeerror-module-pytorch-lightning-has-no-attribute-data-loader/271\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # REQUIRED\n",
    "        return DataLoader(prep_train,collate_fn=self.tuple_batch ,num_workers=0, batch_size=32)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        # OPTIONAL\n",
    "        return DataLoader(prep_val,collate_fn=self.tuple_batch,num_workers=0, batch_size=32)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        # OPTIONAL\n",
    "        return DataLoader(prep_test,collate_fn=self.tuple_batch,num_workers=0, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Almehdi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:91: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: d:\\Almehdi\\Documents\\UPMC\\M1_DAC\\S2\\BIUM\\RecSys_BIUM\\lightning_logs\n",
      "\n",
      "  | Name  | Type      | Params\n",
      "------------------------------------\n",
      "0 | users | Embedding | 30.5 K\n",
      "1 | items | Embedding | 486 K \n",
      "2 | umean | Embedding | 610   \n",
      "3 | imean | Embedding | 9.7 K \n",
      "------------------------------------\n",
      "527 K     Trainable params\n",
      "0         Non-trainable params\n",
      "527 K     Total params\n",
      "2.108     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Almehdi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Almehdi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 215:  69%|██████▉   | 1742/2522 [57:23<25:41,  1.98s/it, loss=0.0394, v_num=0]    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Almehdi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:724: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
      "C:\\Users\\Almehdi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1444: UserWarning: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `test(ckpt_path='best')` to use and best model checkpoint and avoid this warning or `ckpt_path=trainer.checkpoint_callback.last_model_path` to use the last model.\n",
      "  rank_zero_warn(\n",
      "Restoring states from the checkpoint path at d:\\Almehdi\\Documents\\UPMC\\M1_DAC\\S2\\BIUM\\RecSys_BIUM\\lightning_logs\\version_0\\checkpoints\\epoch=214-step=433655.ckpt\n",
      "Loaded model weights from checkpoint at d:\\Almehdi\\Documents\\UPMC\\M1_DAC\\S2\\BIUM\\RecSys_BIUM\\lightning_logs\\version_0\\checkpoints\\epoch=214-step=433655.ckpt\n",
      "C:\\Users\\Almehdi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 631/631 [00:00<00:00, 688.74it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "\n",
    "model = LightningKorenMF(num_users,num_items,50)\n",
    "\n",
    "# most basic trainer, uses good defaults\n",
    "trainer = Trainer()    \n",
    "trainer.fit(model)\n",
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Still got time ?\n",
    "\n",
    "[Take a glance at the documentation](https://williamfalcon.github.io/pytorch-lightning/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
